# -*- coding: utf-8 -*-
"""EN-VI-Medical-Translation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17m-tHIOvqZqxUq27NewRaI8PpZFREk_j
"""

!pip install datasets
!pip install transformers[sentencepiece] datasets
!pip install sacremoses
import os
import pandas as pd
import torch
from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split
from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq
import re
import json
from transformers import MarianMTModel, MarianTokenizer, AutoModelForSeq2SeqLM, AutoTokenizer

from google.colab import drive
drive.mount('/content/drive')



dataset_path = '/content/drive/My Drive/Abdul FYP/Fixed MedEV data.csv'
df = pd.read_csv(dataset_path)

model_path = "/content/drive/MyDrive/Abdul FYP/MarianMT_en_vi_medical_finetuned"
model = MarianMTModel.from_pretrained(model_path)
tokenizer = MarianTokenizer.from_pretrained(model_path)


print("Total sentence pairs:", len(df))
df.head(3)

df['English'] = df['English'].astype(str).str.strip()
df['Viet'] = df['Viet'].astype(str).str.strip()

# Drop rows with empty strings in either column
initial_count = len(df)
df = df[(df['English'] != "") & (df['Viet'] != "")]
df = df.dropna(subset=['English', 'Viet'])
df = df.drop_duplicates(subset=['English', 'Viet'])
cleaned_count = len(df)

print(f"Removed {initial_count - cleaned_count} empty or duplicate pairs. Remaining pairs: {cleaned_count}")

"""code reports how many sentence pairs were removed during cleaning, So should be fairly clean now."""

# Sample 50k pairs for training to manage memory as I think that 340k is way too much for google colab
# Im updating this 100k to see how it manages it(managed well but will keep increasing)
sample_size = 100000
if len(df) > sample_size:
    df_sampled = df.sample(n=sample_size, random_state=42)
else:
    df_sampled = df.copy()

df_sampled = df_sampled.reset_index(drop=True)
print("Sampled sentence pairs:", len(df_sampled))

#from sklearn.model_selection import train_test_split

# 80% train, 20% temp (which will be split into val and test)
train_df, temp_df = train_test_split(df_sampled, test_size=0.2, random_state=42)
# Split temp_df equally into validation and test (10% each of original)
val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)

print(f"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}")

"""splitting the data into testing/training/validation based on metrics ive heard from datamining module."""

max_input_length = 128
max_target_length = 128

print("Tokenizer and model loaded successfully!")

from datasets import Dataset, DatasetDict

# No clue really but is converting Pandas DataFrames to Hugging Face Datasets
train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))
val_dataset   = Dataset.from_pandas(val_df.reset_index(drop=True))
test_dataset  = Dataset.from_pandas(test_df.reset_index(drop=True))

data_dict = DatasetDict({
    "train": train_dataset,
    "validation": val_dataset,
    "test": test_dataset
})


def preprocess_function(examples):
    inputs = [text.strip() for text in examples["English"]]
    targets = [text.strip() for text in examples["Viet"]]
    # Tokenize English text
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)
    # Tokenize Vietnamese text as target
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=max_target_length, truncation=True)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Tokenize the entire dataset (train/val/test)
tokenized_datasets = data_dict.map(preprocess_function, batched=True,
                                   remove_columns=data_dict["train"].column_names)


print("Example tokenized input:", tokenized_datasets['train'][0]['input_ids'][:10])
print("Example tokenized label:", tokenized_datasets['train'][0]['labels'][:10])

"""My dataset should now be tokenized. Each entry in tokenized_datasets has input_ids (tokenized English sentence), attention_mask, and labels (tokenized Vietnamese sentence). The original text columns have been removed."""

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

import torch
from transformers import Seq2SeqTrainingArguments


use_fp16 = torch.cuda.is_available()

training_args = Seq2SeqTrainingArguments(
    output_dir="./en-vi-mt-medical-checkpoints",
    num_train_epochs=4,
    learning_rate=3e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_steps=100,
    predict_with_generate=True,
    fp16=use_fp16,

    save_total_limit=5,
    report_to="none"          #  just so it doesnt do any WandB logging
)

"""Setting test training parameters/arguments."""

from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer
)

# Start training
train_output = trainer.train()

# Define an output path on Drive to save the model
output_dir = "/content/drive/MyDrive/Abdul FYP/MarianMT_en_vi_medical_finetuned"
import os
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

# Save model and tokenizer to the Drive folder
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

print(f"Model and tokenizer saved to {output_dir}")

import json
from datetime import datetime

metadata = {
    "sample_count": len(df_sampled),
    "categories": None,
    "category_counts": None,
    "train_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
}

# when i do categorization was done, include the category distribution
if 'Category' in df_sampled.columns:
    category_counts = df_sampled['Category'].value_counts().to_dict()
    metadata["categories"] = list(category_counts.keys())
    metadata["category_counts"] = category_counts

# metadata ina JSON file
metadata_path = os.path.join(output_dir, "training_metadata.json")
with open(metadata_path, "w") as f:
    json.dump(metadata, f, indent=2)

print(f"Saved training metadata to {metadata_path}")
print("Metadata content:", metadata)

# Step 14: Evaluate BLEU on test set
!pip install -q sacrebleu evaluate

import evaluate
import numpy as np
from tqdm import tqdm

# Load BLEU metric
bleu = evaluate.load("sacrebleu")


model.eval()


preds = []
refs = []

for example in tqdm(tokenized_datasets["test"]):
    input_ids = torch.tensor(example["input_ids"]).unsqueeze(0).to(model.device)
    with torch.no_grad():
        outputs = model.generate(input_ids, max_length=max_target_length)
    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)
    ref = tokenizer.decode(example["labels"], skip_special_tokens=True)

    preds.append(pred)
    refs.append([ref])  # Note: sacrebleu expects list of list

# Compute BLEU
results = bleu.compute(predictions=preds, references=refs)
print(f"\nTest BLEU Score: {results['score']:.2f}")

"""metrics like in the paper, BLEU score for now and then im going to fine tune my model more later. BLEU score initially 38.27"""

meteor = evaluate.load("meteor")

# compute METEOR
results_meteor = meteor.compute(predictions=preds, references=refs)
print(f"METEOR Score: {results_meteor['meteor']:.2f}")

results = bleu.compute(predictions=preds, references=refs)
print(f"\nTest BLEU Score: {results['score']:.2f}")

meteor_score = meteor.compute(predictions=preds, references=refs)
print(f"Test METEOR Score: {meteor_score['meteor']:.4f}")

import csv
from datetime import datetime

# Define evaluation log path (Google Drive location)
eval_log_path = "/content/drive/MyDrive/Abdul FYP/evaluation_log.csv"

# Define current run data
run_data = {
    "Run Name": "Run 2",
    "Sample Count": len(df_sampled),
    "Epochs": training_args.num_train_epochs,
    "Batch Size": training_args.per_device_train_batch_size,
    "Learning Rate": training_args.learning_rate,
    "Training Loss (Final)": train_output.metrics["train_loss"],
    "Validation Loss (Final)": train_output.metrics["eval_loss"] if "eval_loss" in train_output.metrics else "N/A",
    "BLEU Score": results['score'],
    "METEOR Score": meteor_score["meteor"],
    "Training Date": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
}

# Write or append to CSV
import os
file_exists = os.path.isfile(eval_log_path)

with open(eval_log_path, mode='a' if file_exists else 'w', newline='') as file:
    writer = csv.DictWriter(file, fieldnames=run_data.keys())
    if not file_exists:
        writer.writeheader()
    writer.writerow(run_data)

print(f"\nâœ… Run metrics logged to: {eval_log_path}")